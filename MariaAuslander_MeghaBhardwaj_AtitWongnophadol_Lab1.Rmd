---
title: "W271 Group Lab 1"
author: "Maria Auslander, Megha Bhardwaj, Atit Wongnophadol"
subtitle: 'Due 4:00pm Pacific Time Monday June 1 2020'
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
---

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)
knitr::opts_chunk$set(fig.width=8, fig.height=5)
```

**Part 1 (25 points)**

Conduct a thorough EDA of the data set. This should include both graphical and tabular analysis as taught in this course. Output-dump (that is, graphs and tables that don't come with explanations) will result in a very low, if not zero, score. Since the report has a page-limit, you will have to be selective when choosing visuals to illustrate your key points, associated with a concise explanation of the visuals. This EDA should begin with an inspection of the given dataset; examination of anomalies, missing values, potential of top and/or bottom code etc.   

```{r}
# data inspection
challenger<-read.csv("challenger.csv")
str(challenger)
summary(challenger)
```

In the Challenger dataset, there are 23 observations with 5 columns. The column $Flight$ appears to be just for the index of an observation, so it shouldn't have any meaningful information for our study. $Temp$ and $Pressure$ seem to be in a reasonable range and do not contain any missing value. $O.ring$ also contains a reasonable range of integers from 0 and not greater than 6 (the total number of O rings in each flight, which is basically the variable $Number$). In short, by looking at these individual variables separately, they all seem reasonable, do not contain any missing value nor any anomaly. The variables that contain the needed information in this study are $Temp$, $Pressure$ as potential explanatory variables, and $O.ring$ as a dependent variable of a model.

Next, pairwise relationships among $Temp$, $Pressure$ and $O.ring$ are plotted to identify any meaningful pattern and anomaly.


```{r, fig.height = 2.5}
# pairwise relationships between variables of interest
# $Temp$, $Pressure$ and $O.ring$

library(dplyr)
library(ggplot2)

# Distribution of temperature (Temp) by O.ring incidences (O.ring)
# Were those who attended colleage tend to be younger?
ggplot(challenger, aes(factor(O.ring), Temp)) +
geom_boxplot(aes(fill = factor(O.ring))) + 
geom_jitter() +
ggtitle("Temperature and number of incidences on O.ring") + 
theme(plot.title = element_text(lineheight=1, face="bold")) 

# Distribution of pressure (Pressure) by O.ring incidences (O.ring)
ggplot(challenger, aes(factor(O.ring), Pressure)) +
geom_boxplot(aes(fill = factor(O.ring))) + 
geom_jitter() +
ggtitle("Pressure and number of incidences on O.ring") + 
theme(plot.title = element_text(lineheight=1, face="bold")) 

# Relationship between Temp and Pressure
plot(challenger$Temp, challenger$Pressure, 
     main="Temp vs Pressure",
     xlab="Temp", ylab="Pressure", pch=19)

# abline(lm(challenger$Pressure ~ challenger$Temp), col="red")


# 3d scatter plot for the relatinoship between all 3 variables
library("scatterplot3d")

scatterplot3d(challenger$Temp, challenger$Pressure, challenger$O.ring, pch = 19, type="h",
              xlab = "Temperature", ylab = "Pressure", zlab = "O.ring incidence")

```

From the graphs above, there is an obvious relationship between temperature and O.ring incidence, whereas temperature lower than 60 some degree tends to associate with at least one O.ring incidence. 
In terms of pressure, O.ring seems to be able to withstand a wide range of pressure, as can be observed by the density around both end of the pressure spectrum from low of 50 to high of 200. However, there is slight concerning evidence that 6 flights with at least one occurred O.ring incidence were observed with the high pressure level; there were two exception flights in which O.ring incidence occurred with the pressure below 50. So the relationship between pressure and O.ring incidence might actually exist, but given the scant data it cannot be ascertained.

The next scatter plot between temperature and pressure don't give much information. There is no apparent relationship that can be drawn from the data.

The last 3d plot attempts to draw a relationship between an interaction between temperature and pressure and the resulting O.ring incidence. There is no apparent evidence that an interaction effect between temperature and pressure on O.ring incidence exists.

In summary, there is an obvious relationship between temperature and O.ring incidence that justifies further exploration and model formulation. A light evidence exists for a relationship between pressure and O.ring incidence; a hypothesis on pressure factor may be formed and tested to verify if it plays a significant role in explaining O.ring incidence.

The following matrix shows each column name as well as the number of missing values, the minimum column value, the median column value, and the maximum column value (respectively). Following the matrix, each histogram of values for columns were shown.


```{r, fig.height = 3}
# par(mfrow = c(2,ncol(challenger)%/%2))

columns<-names(challenger)

challenger.eda.matrix <- matrix(data=NA,nrow=length(challenger),ncol=5)

counter=1
for (col in columns) {
    show(hist(challenger[[col]],main=col,xlab=col))
    num_missing<-sum(is.na(challenger[[col]]))
    maxn<-max(challenger[[col]])
    medn<-median(challenger[[col]])
    minn<-min(challenger[[col]])
    challenger.eda.matrix[counter,] = c(col,num_missing,minn,medn,maxn)
    counter = counter+1
}
```
Looking at the histograms and summary statistics, the low pressure data points may be anomalies and a cause for concern. The large O.ring values additionally seem to be anomalies and potentially a cause for concern. Number maintains a value of 6 throughout the dataset.

The data below shows the number of missing values for each column within the dataset, there is no missing data.
```{r}
challenger.eda.matrix[,2]
```


**Part 2 (20 points)** 

Answer the following from Question 4 of Bilder and Loughin Section 2.4 Exercises (page 129):

The response variable is O.ring, and the explanatory variables are Temp and  Pressure. Complete the following: 

(a) The authors use logistic regression to estimate the probability an O-ring will fail. In order to use this model, the authors needed to assume that each O-ring is independent for each launch. Discuss why this assumption is necessary and the potential problems with it. Note that a subsequent analysis helped to alleviate the authorsâ€™ concerns about independence.

Logistic regression assumes a binomial distribution, in the case of binomial distribution, independence of trials is a necessary assumption. The issue is that the O-rings are on every rocket (6/rocket), so there may be some dependencies on each other given they are located on the same entity.


(b) Estimate the logistic regression model using the explanatory variables in a linear form.

There are two possible models--binomial and binary.

#### Binomial Regression
```{r}
challenger$percent.O.ring.fail<- with(challenger, O.ring/Number)
challenger_glm<-glm(percent.O.ring.fail~Temp+Pressure,
                  family=binomial(link="logit")
                  ,data=challenger)
summary(challenger_glm)
```
$$ logit(\hat\pi)=\beta_0+\beta_1 x_1 + \beta_2 x_2 $$
$$logit(\hat\pi)=2.520195-0.098297 x_1 + 0.008484 x_2 $$
Where $x_1=Temp$ and $x_2=Pressure$


#### Binary Regression

In the case of binary regression, we assume at least one O.ring failed is countered as a failed flight (1) while no failures is counted as a successful flight (0).

```{r}
challenger_glm_binary<-glm(O.ring >0 ~Temp+Pressure,
                  family=binomial(link="logit")
                  ,data=challenger)
summary(challenger_glm_binary)
```


(c) Perform LRTs to judge the importance of the explanatory variables in the model. 

(??? - Not sure if these are right, p values appear to be too high when compared to the model)

```{r}
library("lmtest")
library("car")
temp_only<-glm(percent.O.ring.fail~Temp,
                  family=binomial(link="logit"),
                  data=challenger)
pressure_only<-glm(percent.O.ring.fail~Pressure,
                  family=binomial(link="logit"),
                  data=challenger)

Anova(challenger_glm) 
anova(temp_only,challenger_glm, test = "Chisq") 
anova(pressure_only,challenger_glm, test = "Chisq") 
```

```{r}
# LRTs
# Given other variables in model, "test = 'LR' is the default (type II)
Anova(challenger_glm_binary, test = "LR")  

# Sequential testing of variables (type I)
anova(challenger_glm_binary, test = "Chisq")
```


(d) The authors chose to remove Pressure from the model based on the LRTs. Based on your results, discuss why you think this was done. Are there any potential problems with removing this variable?


`Pressure` was likely taken from the model because the according p-value was large (p>alpha=0.05) while the p-value for `Temp` was small (p<alpha=0.05). There is not enough evidence to assume that `Pressure` is important in the explanatory model, however, removing `Pressure` from the model may be an issue if `Pressure` could be the part of an interaction term or a transformation in a future model.


**Part 3 (35 points)**

Answer the following from Question 5 of Bilder and Loughin Section 2.4 Exercises (page 129-130):

Continuing Exercise 4, consider the simplified model $logit(\pi) = \beta_0 +  \beta_1 Temp$, where $\pi$ is the probability of an O-ring failure. Complete the following:

(a) Estimate the model.

```{r}
challenger_glm_2<-glm(percent.O.ring.fail~Temp,
                  family=binomial(link="logit"),
                  data=challenger)
summary(challenger_glm_2)
```
$$ logit(\hat\pi)=\beta_0+\beta_1 x_1$$
$$ logit(\hat\pi)=5.0850-0.1156 x_1$$

Where $x_1=Temp$


```{r}
# binary regression (i.e., at least one O.ring failed is countered as a failed flight)
challenger_glm_binary<-glm(O.ring >0 ~Temp,
                  family=binomial(link="logit")
                  ,data=challenger)
summary(challenger_glm_binary)
```

(b) Construct two plots: (1) $\pi$ vs. Temp and (2) Expected number of failures vs. Temp. Use a temperature range of 31Â° to 81Â° on the x-axis even though the minimum temperature in the data set was 53Â°.

```{r}
temp <-31:81
pi_val <- predict(challenger_glm_2, list(Temp = temp),type="response")
plot(x=temp, y=pi_val,type="l",ylab=expression(pi),main="Pi vs. Temp")

plot(x=temp, y=pi_val*6,type="l",ylab=expression(pi),main="Expected Number of Failures vs. Temp")

```


(c) Include the 95% Wald confidence interval bands for $\pi$ on the plot. Why are the bands much wider for lower temperatures than for higher temperatures?

(???- CI lines seem off, particularly around extremes)
```{r}
temp <-31:81
pi_val <- predict(challenger_glm_2, list(Temp = temp),type="response")
plot(x=temp, y=pi_val,type="l",ylab=expression(pi),main="Pi vs. Temp",ylim=c(0,1))

inverse_logit <- function(x){
  exp(x)/(1+exp(x))
}

#Wald interval
predicted <- predict(challenger_glm_2, list(Temp = temp), se.fit = TRUE)

pred0 <- predicted$fit
pred <- inverse_logit(predicted$fit)
alpha <- 0.05
sc <- abs(qnorm(alpha/2))  ## Normal approx. to likelihood
lwr<-inverse_logit(pred0-sc*predicted$se.fit)
upr<-inverse_logit(pred0+sc*predicted$se.fit)

lines(temp, upr, col="blue")
lines(temp, lwr, col="blue")

plot(x=temp, y=pi_val*6,type="l",ylab=expression(pi),main="Expected Number of Failures vs. Temp",ylim=c(0,6))
lines(temp, upr*6, col="blue")
lines(temp, lwr*6, col="blue")

```

There are fewer estimations for the lower temperatures in the dataset, so there is higher uncertainty at lower temperatures. This higher uncertainty also occurs at the upper end of the data with higher temperatures as there are less estimates for higher temperatures as well.

(d) The temperature was 31Â° at launch for the Challenger in 1986. Estimate the probability of an O-ring failure using this temperature, and compute a corresponding confidence interval. Discuss what assumptions need to be made in order to apply the inference procedures.

(??? - values seem to extreme)
```{r}
single_pred<-predict(challenger_glm_2, list(Temp = c(31)),type="link",se.fit=TRUE)

pred0 <- single_pred$fit
pred <- inverse_logit(single_pred$fit)
ci <- 0.90
sc <- abs(qnorm((1-ci)/2))  ## Normal approx. to likelihood
lwr=inverse_logit(pred0-(sc*single_pred$se.fit))
upr=inverse_logit(pred0+(sc*single_pred$se.fit))

lwr
pred
upr
```

We need to assume the same trend that occurs at the temperature range available in the dataset (53,81) also occurs at the lower temperature, that the model is still applicable.


(e) Rather than using Wald or profile LR intervals for the probability of failure, Dalal et al. (1989) use a parametric bootstrap to compute intervals. Their process was to (1) simulate a large number of data sets (n = 23 for each) from the estimated model of  Temp; (2) estimate new models for each data set, say and (3) compute  at a specific temperature of interest. The authors used the 0.05 and 0.95 observed quantiles from the  simulated distribution as their 90% confidence interval limits. Using the parametric bootstrap, compute 90% confidence intervals separately at temperatures of 31Â° and 72Â°.27

```{r}
# bootstrap function for simulating pi.hat
bootstrap <- function(temp){

  # bootstrap resampling of the dataset
  challenger.rep <- sample_frac(challenger, replace = TRUE)
  
  # fit the model
  challenger.rep$percent.O.ring.fail <- with(challenger.rep, O.ring/Number)
  mod.rep.fit <- suppressWarnings(glm(percent.O.ring.fail~Temp,
                    family=binomial(link="logit"),
                    data=challenger.rep))
  
  # predict pi.hat
  pi.hat <- predict(mod.rep.fit, list(Temp = temp), type="response")
  
  return(pi.hat)
}

# number of trials
N <- 100000

# save the list of simulated pi.hat
rep.pi.hat.31 <- replicate(N, bootstrap(31))
rep.pi.hat.72 <- replicate(N, bootstrap(72))

# 90% confidence interval of pi.hat
quantile(rep.pi.hat.31, c(.05, .50, .95)) # CI+median at 31 degrees
quantile(rep.pi.hat.72, c(.05, .50, .95)) # CI+median at 72 degrees

# average pi.hat
mean(rep.pi.hat.31)
mean(rep.pi.hat.72)
```

The parametric bootstrap method yield the following confidence intervals [0.21123, 0.99415] and [0.00571, 0.08427] for temperature 31 degree and 72 degree respectively.


(f) Determine if a quadratic term is needed in the model for the temperature.

```{r}
quad_model<-glm(percent.O.ring.fail~Temp+I(Temp**2),
                  family=binomial
                  ,data=challenger)
summary(quad_model)

temp_only<-glm(percent.O.ring.fail~Temp,
                  family=binomial,
                  data=challenger)

anova(temp_only,quad_model,test="LR")
```

The p-value of the likelihood ratio test comparing the model with a quadratic term of temperature and without a quadratic term of temperature is large (>0.05), indicating that there is not enough evidence suggesting a quadratic term is needed in the model.

**Part 4 (10 points)**

With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions.  Would you use the linear regression model or binary logistic regression in this case?  Explain why.

In the final model, we use only $Temp$ as a independent variable as $Pressure$ was not statistically significant and there is no evidence to suggest a quadratic term of $Temp$ is needed. We use the dependent variable from the binomial model (`percent.O.ring.fail`) in the linear model, as the continuous nature of the variable is more conducive to the linear model.

```{r}
linear_model_temp_only<-lm(percent.O.ring.fail~Temp,data=challenger)
summary(linear_model_temp_only)
```

```{r}
library(gvlma)
gvmodel <- gvlma(linear_model_temp_only)
summary(gvmodel)
```

Before checking assumptions through individual plots, we ran the Global Validation of Linear Models Assumptions (gvlma) test. Through this test we see violations of the global stat, skewness, and kurtosis assumptions. We fail to reject the assumptions of the link function and heteroscedasticity. 

### CLM Assumptions

#### Linear Population Model

Under the linear population model assumption, the relationship between dependent and independent variables is meant to be linear. Looking at the global stat p-value (1.670532e-05) it appears this is not the case. This p-value indicates a non-linear relationship between variables. 

#### Random Sampling

The random sampling assumption is best tested by reviewing the methods of data collection for the dataset. If the data was collected in a random, unbiased way the assumption would be fulfilled, if not, the assumption would be rejected.

#### Zero Conditional Mean
```{r}
plot(linear_model_temp_only, which = 1)
```

Looking at the residual vs. fitted plot above, there is a violation of the zero conditional mean assumption. In the case of the zero conditional mean assumption being fulfilled, we would expect residuals to be spaced evenly around the 0 line, this is not the case. Through the graph it appears there may be some sort of curved relationship present.

#### Homoscedasticity

```{r}
plot(linear_model_temp_only, which = 3)
```

The homoscedasticity assumption states that the variance of a residual should be about the same for any value of x. The scale-location plot shows a violation of this assumption. If the assumption were fulfilled we would expect the points to be spaced evenly and randomly above and below the line, however, in this case there appears to be evidence of a curved relationship.

#### Normality of Errors
```{r}
install.packages('olsrr')
library(olsrr)
plot(linear_model_temp_only, which = 2)
hist(linear_model_temp_only$residuals)
ols_test_normality(linear_model_temp_only)
```

Looking at the normal Q-Q plot, the histogram of residuals and the Anderson-Darling test statistic the normality of errors assumption is not satisfied. The normal Q-Q plot shows many residuals straying far from the line, indicating a distribution that is not normal. The histogram of residuals does not show a normal relationship with residuals distributed around the mean (which following CLM assumptions should be 0). Additionally the Anderson-Darling test statistic has a p-value of 0.0046 < 0.05, indicating that the null hypothesis of normality of errors is violated. Using the skewness assumption test in the glvma (which showed cause for violation), it appears the model would need to be transformed to meet the normality assumption.

**Part 5 (10 points)**

Interpret the main result of your final model in terms of both odds and probability of failure. Summarize the final result with respect to the question(s) being asked and key takeaways from the analysis.